{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trip_df = pd.read_csv('../data/trip_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data = cleaned_trip_df\n",
    "episode_data = episode_data[['hack_license', 'pickup_datetime', 'dropoff_datetime', \n",
    "                            'pickup_taxizone_id', 'dropoff_taxizone_id', \n",
    "                            'total_amount', 'first_pickup', 'last_dropoff']].copy()\n",
    "episode_data.sort_values(['hack_license', 'pickup_datetime'], inplace=True)\n",
    "episode_data['pickup_datetime'] = pd.to_datetime(episode_data['pickup_datetime'])\n",
    "episode_data['dropoff_datetime'] = pd.to_datetime(episode_data['dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only data points of full shift \n",
    "A complete shift must have both the first pickup and the last drop off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data['mask_start'] = np.where(episode_data[\"first_pickup\"] == 1, 1, np.nan)\n",
    "episode_data['mask_start'] = episode_data.groupby('hack_license')['mask_start'].ffill()\n",
    "episode_data['mask_end'] = np.where(episode_data[\"last_dropoff\"] == 1, 1, np.nan)\n",
    "episode_data['mask_end'] = episode_data.groupby('hack_license')['mask_end'].bfill()\n",
    "episode_data['mask'] = np.where((episode_data[\"mask_start\"] == 1) & (episode_data[\"mask_end\"] == 1), 1, np.nan)\n",
    "episode_data.dropna(subset=['mask'], inplace=True)\n",
    "episode_data.drop(columns=['mask_start', 'mask_end', 'mask'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique episode id\n",
    "One shift of a driver is assgined with a unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data['episode'] = np.where(episode_data[\"first_pickup\"] == 1, episode_data[\"first_pickup\"].index, np.nan)\n",
    "episode_data['episode'] = episode_data.groupby('hack_license')['episode'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all time columns to time index\n",
    "depending on `delta_t`. First round and convert using `interval_index_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_index_table = pd.read_csv('../data/interval_index_table_0.csv')\n",
    "interval_index_table['interval'] = pd.to_datetime(interval_index_table['interval']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 15\n",
    "round_by = '{}min'.format(delta_t)\n",
    "episode_data['pickup_datetime_interval'] = episode_data['pickup_datetime'].dt.round(round_by).dt.time\n",
    "episode_data['dropoff_datetime_interval'] = episode_data['dropoff_datetime'].dt.round(round_by).dt.time\n",
    "\n",
    "## convert DO interval to time index\n",
    "current_conversion = dict(zip(interval_index_table.interval, interval_index_table[f'time_index_{delta_t}m']))\n",
    "episode_data['pickup_datetime_index'] = [current_conversion[t] for t in episode_data['pickup_datetime_interval']]\n",
    "episode_data['dropoff_datetime_index'] = [current_conversion[t] for t in episode_data['dropoff_datetime_interval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_data = episode_data[['pickup_taxizone_id', 'dropoff_taxizone_id', \n",
    "#                              'pickup_datetime_index', 'dropoff_datetime_index',\n",
    "#                              'total_amount', 'episode']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapse Immediate trips \n",
    "\n",
    "trips/transition must collaspe if current action = 0 and there is immediate transition after the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## action that bring from current row to before the transition of the next row\n",
    "episode_data['pickup_taxizone_id_next'] = episode_data.groupby(['hack_license'])['pickup_taxizone_id'].shift(-1)\n",
    "episode_data['action'] = np.where(episode_data['dropoff_taxizone_id']  == episode_data['pickup_taxizone_id_next'], \n",
    "                                  0, episode_data['pickup_taxizone_id_next'])\n",
    "\n",
    "## immediate = 1 if current row is immediate of prev row\n",
    "episode_data['pickup_datetime_index_next'] = episode_data.groupby(['hack_license'])['pickup_datetime_index'].shift(-1)\n",
    "episode_data['same_time_interval'] = np.where(episode_data['pickup_datetime_index_next']  == episode_data['dropoff_datetime_index'], \n",
    "                                  1, 0)\n",
    "episode_data['finished'] = np.where((episode_data['same_time_interval']==1) & (episode_data['action']==0), \n",
    "                                  np.nan, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data['state_loc'] = episode_data['dropoff_taxizone_id']\n",
    "episode_data['state_time'] = episode_data['dropoff_datetime_index']\n",
    "episode_data.dropna(subset=['finished'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only relevant columns\n",
    "* state_loc = current location\n",
    "* state_time = current time index\n",
    "* action = next action to take \n",
    "* total_amount = current reward (the previous reward before the R in SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data = episode_data[['state_loc', 'state_time', 'action', 'total_amount', 'episode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/historical_for_SARSA.pickle', 'wb') as handle:\n",
    "    pickle.dump(episode_data, handle)\n",
    "\n",
    "## For interpretable read. Suffering data loss.\n",
    "episode_data.to_csv('../data/historical_for_SARSA.pickle', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pickle\n",
    "# with open('../data/historical_for_SARSA.pickle', 'rb') as handle:\n",
    "#     episode_data = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
