{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import NYCEnv\n",
    "\n",
    "from stable_baselines import ACKTR\n",
    "from stable_baselines.common.cmd_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/emp_Q_shA.pkl', 'rb') as f:\n",
    "    emp_Q_shA = pickle.load(f)\n",
    "with open('data/emp_Q_shB.pkl', 'rb') as f:\n",
    "    emp_Q_shB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_reposition(obs, shifts='A'):\n",
    "    obs = tuple(obs)\n",
    "    if shifts == 'A' and obs in emp_Q_shA.keys():\n",
    "        reposition = np.argmax(emp_Q_shA[obs])\n",
    "    elif shifts == 'B' and obs in emp_Q_shB.keys():\n",
    "        reposition = np.argmax(emp_Q_shB[obs])\n",
    "    else:\n",
    "        reposition = obs[0]\n",
    "    return reposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NYCEnv(delta_t=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>103.171706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>108.164884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.078565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84.351039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>190.311163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>334.270996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean   103.171706\n",
       "std    108.164884\n",
       "min     -2.078565\n",
       "25%      0.000000\n",
       "50%     84.351039\n",
       "75%    190.311163\n",
       "max    334.270996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "rewards = np.zeros(epochs)\n",
    "n_steps = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    obs = env.reset()\n",
    "    obs, reward, done, info = env.step(obs[0])\n",
    "    for step in range(1, n_steps):\n",
    "        reposition = optimal_reposition(obs, shifts='A')\n",
    "        obs, reward, done, info = env.step(reposition)\n",
    "        if reward == env.TERMINATE_PENALTY:\n",
    "            rewards[epoch] = env.total_rewards - reward\n",
    "            break\n",
    "        if done:\n",
    "            rewards[epoch] = env.total_rewards\n",
    "            break\n",
    "        elif step == n_steps-1:\n",
    "            rewards[epoch] = env.total_rewards\n",
    "pd.DataFrame(rewards).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>153.731610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>159.635944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.417404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>114.877018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>318.827105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>452.984540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean   153.731610\n",
       "std    159.635944\n",
       "min     -4.417404\n",
       "25%      0.000000\n",
       "50%    114.877018\n",
       "75%    318.827105\n",
       "max    452.984540"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no repositioning\n",
    "epochs = 100\n",
    "rewards = np.zeros(epochs)\n",
    "n_steps = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    obs = env.reset()\n",
    "    obs, reward, done, info = env.step(obs[0])\n",
    "    for step in range(1, n_steps):\n",
    "        obs, reward, done, info = env.step(obs[0])\n",
    "        if reward == env.TERMINATE_PENALTY:\n",
    "            rewards[epoch] = env.total_rewards - reward\n",
    "            break\n",
    "        if done:\n",
    "            rewards[epoch] = env.total_rewards\n",
    "            break\n",
    "        elif step == n_steps-1:\n",
    "            rewards[epoch] = env.total_rewards\n",
    "pd.DataFrame(rewards).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.05      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -0.00338  |\n",
      "| fps                | 23        |\n",
      "| nupdates           | 1         |\n",
      "| policy_entropy     | 5.58      |\n",
      "| policy_loss        | -5.57e+04 |\n",
      "| total_timesteps    | 20        |\n",
      "| value_loss         | 9.99e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 313       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 4.56      |\n",
      "| policy_loss        | -4.25e+04 |\n",
      "| total_timesteps    | 2000      |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 200       |\n",
      "| policy_entropy     | 4.77      |\n",
      "| policy_loss        | -4.61e+04 |\n",
      "| total_timesteps    | 4000      |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 332       |\n",
      "| nupdates           | 300       |\n",
      "| policy_entropy     | 4.95      |\n",
      "| policy_loss        | -5.33e+04 |\n",
      "| total_timesteps    | 6000      |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 331       |\n",
      "| nupdates           | 400       |\n",
      "| policy_entropy     | 4.99      |\n",
      "| policy_loss        | -4.64e+04 |\n",
      "| total_timesteps    | 8000      |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -9.54e-07 |\n",
      "| fps                | 326       |\n",
      "| nupdates           | 500       |\n",
      "| policy_entropy     | 4.91      |\n",
      "| policy_loss        | -5.18e+04 |\n",
      "| total_timesteps    | 10000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 326       |\n",
      "| nupdates           | 600       |\n",
      "| policy_entropy     | 4.8       |\n",
      "| policy_loss        | -5.01e+04 |\n",
      "| total_timesteps    | 12000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.05      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -5.13e-06 |\n",
      "| fps                | 328       |\n",
      "| nupdates           | 700       |\n",
      "| policy_entropy     | 4.92      |\n",
      "| policy_loss        | -4.93e+04 |\n",
      "| total_timesteps    | 14000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 331       |\n",
      "| nupdates           | 800       |\n",
      "| policy_entropy     | 4.91      |\n",
      "| policy_loss        | -5.12e+04 |\n",
      "| total_timesteps    | 16000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -8.23e-06 |\n",
      "| fps                | 329       |\n",
      "| nupdates           | 900       |\n",
      "| policy_entropy     | 4.87      |\n",
      "| policy_loss        | -4.49e+04 |\n",
      "| total_timesteps    | 18000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1         |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 330       |\n",
      "| nupdates           | 1000      |\n",
      "| policy_entropy     | 4.84      |\n",
      "| policy_loss        | -4.97e+04 |\n",
      "| total_timesteps    | 20000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -9.9e+03  |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 329       |\n",
      "| nupdates           | 1100      |\n",
      "| policy_entropy     | 4.89      |\n",
      "| policy_loss        | -4.79e+04 |\n",
      "| total_timesteps    | 22000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.05      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -3.1e-06  |\n",
      "| fps                | 328       |\n",
      "| nupdates           | 1200      |\n",
      "| policy_entropy     | 5.02      |\n",
      "| policy_loss        | -5.35e+04 |\n",
      "| total_timesteps    | 24000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 327       |\n",
      "| nupdates           | 1300      |\n",
      "| policy_entropy     | 4.93      |\n",
      "| policy_loss        | -5.07e+04 |\n",
      "| total_timesteps    | 26000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 326       |\n",
      "| nupdates           | 1400      |\n",
      "| policy_entropy     | 4.95      |\n",
      "| policy_loss        | -5.08e+04 |\n",
      "| total_timesteps    | 28000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 320       |\n",
      "| nupdates           | 1500      |\n",
      "| policy_entropy     | 4.95      |\n",
      "| policy_loss        | -4.83e+04 |\n",
      "| total_timesteps    | 30000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 315       |\n",
      "| nupdates           | 1600      |\n",
      "| policy_entropy     | 4.95      |\n",
      "| policy_loss        | -4.79e+04 |\n",
      "| total_timesteps    | 32000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | 0         |\n",
      "| fps                | 315       |\n",
      "| nupdates           | 1700      |\n",
      "| policy_entropy     | 5.03      |\n",
      "| policy_loss        | -5.22e+04 |\n",
      "| total_timesteps    | 34000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 315       |\n",
      "| nupdates           | 1800      |\n",
      "| policy_entropy     | 5.11      |\n",
      "| policy_loss        | -5.08e+04 |\n",
      "| total_timesteps    | 36000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 315       |\n",
      "| nupdates           | 1900      |\n",
      "| policy_entropy     | 5.08      |\n",
      "| policy_loss        | -4.93e+04 |\n",
      "| total_timesteps    | 38000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 315       |\n",
      "| nupdates           | 2000      |\n",
      "| policy_entropy     | 5.01      |\n",
      "| policy_loss        | -4.89e+04 |\n",
      "| total_timesteps    | 40000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1         |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 316       |\n",
      "| nupdates           | 2100      |\n",
      "| policy_entropy     | 4.98      |\n",
      "| policy_loss        | -5.25e+04 |\n",
      "| total_timesteps    | 42000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 316       |\n",
      "| nupdates           | 2200      |\n",
      "| policy_entropy     | 4.74      |\n",
      "| policy_loss        | -4.69e+04 |\n",
      "| total_timesteps    | 44000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -2.03e-06 |\n",
      "| fps                | 317       |\n",
      "| nupdates           | 2300      |\n",
      "| policy_entropy     | 4.88      |\n",
      "| policy_loss        | -5.26e+04 |\n",
      "| total_timesteps    | 46000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 318       |\n",
      "| nupdates           | 2400      |\n",
      "| policy_entropy     | 4.9       |\n",
      "| policy_loss        | -5.13e+04 |\n",
      "| total_timesteps    | 48000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 318       |\n",
      "| nupdates           | 2500      |\n",
      "| policy_entropy     | 4.93      |\n",
      "| policy_loss        | -5.02e+04 |\n",
      "| total_timesteps    | 50000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ACKTR('MlpPolicy', env, verbose=1).learn(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs= [[41 26]]\n",
      "==========\n",
      "Step 1\n",
      "Action:  [107]\n",
      "obs= [[144  26]] reward= [-10000.] done= [ True]\n",
      "Current taxi zone: 144, time: 26, reward: 0.00\n",
      "==========\n",
      "Goal reached! reward= [-10000.]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "print(\"obs=\", obs)\n",
    "print('==========')\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "    env.render(mode='console')\n",
    "    print('==========')\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
