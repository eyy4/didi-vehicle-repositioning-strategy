{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import NYCEnv\n",
    "\n",
    "from stable_baselines import ACKTR\n",
    "from stable_baselines.common.cmd_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NYCEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current taxi zone: 230, time: 48, reward: 0.00\n",
      "Box(1, 263, (2,), int32)\n",
      "Discrete(264)\n",
      "220\n",
      "[230  48]\n",
      "==========\n",
      "Step 1\n",
      "Action 230\n",
      "Current taxi zone: 186, time: 49, reward: 6.19\n",
      "==========\n",
      "Step 2\n",
      "Action 186\n",
      "Current taxi zone: 230, time: 50, reward: 13.87\n",
      "==========\n",
      "Step 3\n",
      "Action 230\n",
      "Current taxi zone: 43, time: 51, reward: 21.34\n",
      "==========\n",
      "Step 4\n",
      "Action 43\n",
      "Current taxi zone: 237, time: 52, reward: 28.87\n",
      "==========\n",
      "Step 5\n",
      "Action 237\n",
      "Current taxi zone: 161, time: 53, reward: 37.92\n",
      "==========\n",
      "Step 6\n",
      "Action 161\n",
      "Current taxi zone: 237, time: 54, reward: 44.70\n",
      "==========\n",
      "Step 7\n",
      "Action 237\n",
      "Current taxi zone: 125, time: 57, reward: 65.47\n",
      "==========\n",
      "Step 8\n",
      "Action 125\n",
      "Current taxi zone: 125, time: 58, reward: 65.47\n",
      "==========\n",
      "Step 9\n",
      "Action 125\n",
      "Current taxi zone: 234, time: 59, reward: 74.60\n",
      "==========\n",
      "Step 10\n",
      "Action 234\n",
      "Current taxi zone: 12, time: 61, reward: 87.90\n",
      "==========\n",
      "Step 11\n",
      "Action 12\n",
      "Current taxi zone: 144, time: 63, reward: 86.63\n",
      "==========\n",
      "Step 12\n",
      "Action 144\n",
      "Current taxi zone: 161, time: 65, reward: 102.68\n",
      "==========\n",
      "Step 13\n",
      "Action 161\n",
      "Current taxi zone: 140, time: 66, reward: 113.09\n",
      "==========\n",
      "Step 14\n",
      "Action 140\n",
      "Current taxi zone: 161, time: 68, reward: 126.24\n",
      "==========\n",
      "Step 15\n",
      "Action 161\n",
      "Current taxi zone: 163, time: 69, reward: 132.59\n",
      "==========\n",
      "Step 16\n",
      "Action 163\n",
      "Current taxi zone: 246, time: 70, reward: 144.67\n",
      "==========\n",
      "Step 17\n",
      "Action 246\n",
      "Current taxi zone: 48, time: 71, reward: 152.32\n",
      "==========\n",
      "Step 18\n",
      "Action 48\n",
      "Current taxi zone: 107, time: 73, reward: 164.73\n",
      "==========\n",
      "Step 19\n",
      "Action 107\n",
      "Current taxi zone: 234, time: 74, reward: 171.10\n",
      "==========\n",
      "Step 20\n",
      "Action 234\n",
      "Current taxi zone: 164, time: 75, reward: 178.43\n",
      "==========\n",
      "Step 21\n",
      "Action 164\n",
      "Current taxi zone: 68, time: 76, reward: 186.98\n",
      "==========\n",
      "Step 22\n",
      "Action 68\n",
      "Current taxi zone: 234, time: 77, reward: 194.94\n",
      "==========\n",
      "Step 23\n",
      "Action 234\n",
      "Current taxi zone: 229, time: 79, reward: 206.57\n",
      "==========\n",
      "Step 24\n",
      "Action 229\n",
      "Current taxi zone: 87, time: 81, reward: 223.01\n",
      "==========\n",
      "Step 25\n",
      "Action 87\n",
      "Current taxi zone: 239, time: 84, reward: 245.26\n",
      "==========\n",
      "Step 26\n",
      "Action 239\n",
      "Current taxi zone: 162, time: 86, reward: 258.33\n",
      "==========\n",
      "Step 27\n",
      "Action 162\n",
      "Current taxi zone: 170, time: 87, reward: 265.36\n",
      "==========\n",
      "Step 28\n",
      "Action 170\n",
      "Current taxi zone: 48, time: 88, reward: 275.99\n",
      "==========\n",
      "Step 29\n",
      "Action 48\n",
      "Current taxi zone: 132, time: 94, reward: 325.77\n",
      "==========\n",
      "Step 30\n",
      "Action 132\n",
      "Current taxi zone: 132, time: 97, reward: 325.77\n",
      "==========\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "print(obs)\n",
    "print('==========')\n",
    "\n",
    "n_steps = 100\n",
    "for step in range(n_steps):\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    print(\"Action {}\".format(obs[0]))\n",
    "    obs, reward, done, info = env.step(obs[0])\n",
    "    env.render()\n",
    "    print('==========')\n",
    "    if done:\n",
    "        print(\"done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Ananconda\\install\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1         |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 31        |\n",
      "| nupdates           | 1         |\n",
      "| policy_entropy     | 5.58      |\n",
      "| policy_loss        | -5.58e+04 |\n",
      "| total_timesteps    | 20        |\n",
      "| value_loss         | 1e+08     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -0.00115  |\n",
      "| fps                | 340       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 5.2       |\n",
      "| policy_loss        | -5.02e+04 |\n",
      "| total_timesteps    | 2000      |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 341       |\n",
      "| nupdates           | 200       |\n",
      "| policy_entropy     | 5.03      |\n",
      "| policy_loss        | -4.84e+04 |\n",
      "| total_timesteps    | 4000      |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -0.000348 |\n",
      "| fps                | 337       |\n",
      "| nupdates           | 300       |\n",
      "| policy_entropy     | 5.07      |\n",
      "| policy_loss        | -5.31e+04 |\n",
      "| total_timesteps    | 6000      |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 332       |\n",
      "| nupdates           | 400       |\n",
      "| policy_entropy     | 5.09      |\n",
      "| policy_loss        | -5.47e+04 |\n",
      "| total_timesteps    | 8000      |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -0.000262 |\n",
      "| fps                | 329       |\n",
      "| nupdates           | 500       |\n",
      "| policy_entropy     | 5.1       |\n",
      "| policy_loss        | -4.86e+04 |\n",
      "| total_timesteps    | 10000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 326       |\n",
      "| nupdates           | 600       |\n",
      "| policy_entropy     | 5.16      |\n",
      "| policy_loss        | -5.04e+04 |\n",
      "| total_timesteps    | 12000     |\n",
      "| value_loss         | 9.93e+07  |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| ep_len_mean        | 1.01     |\n",
      "| ep_reward_mean     | -9.9e+03 |\n",
      "| explained_variance | nan      |\n",
      "| fps                | 326      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 5.13     |\n",
      "| policy_loss        | -5.1e+04 |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 9.93e+07 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 800       |\n",
      "| policy_entropy     | 4.96      |\n",
      "| policy_loss        | -4.97e+04 |\n",
      "| total_timesteps    | 16000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | 1.64e-05  |\n",
      "| fps                | 323       |\n",
      "| nupdates           | 900       |\n",
      "| policy_entropy     | 5.08      |\n",
      "| policy_loss        | -4.91e+04 |\n",
      "| total_timesteps    | 18000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 1000      |\n",
      "| policy_entropy     | 5.07      |\n",
      "| policy_loss        | -5.25e+04 |\n",
      "| total_timesteps    | 20000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 1100      |\n",
      "| policy_entropy     | 5.12      |\n",
      "| policy_loss        | -5.01e+04 |\n",
      "| total_timesteps    | 22000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 1200      |\n",
      "| policy_entropy     | 5.15      |\n",
      "| policy_loss        | -5.02e+04 |\n",
      "| total_timesteps    | 24000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -3.93e-06 |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 1300      |\n",
      "| policy_entropy     | 5.19      |\n",
      "| policy_loss        | -5.17e+04 |\n",
      "| total_timesteps    | 26000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1         |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1400      |\n",
      "| policy_entropy     | 5.14      |\n",
      "| policy_loss        | -4.92e+04 |\n",
      "| total_timesteps    | 28000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1500      |\n",
      "| policy_entropy     | 5.1       |\n",
      "| policy_loss        | -5.06e+04 |\n",
      "| total_timesteps    | 30000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -9.9e+03  |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1600      |\n",
      "| policy_entropy     | 5.07      |\n",
      "| policy_loss        | -4.54e+04 |\n",
      "| total_timesteps    | 32000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -9.9e+03  |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1700      |\n",
      "| policy_entropy     | 5.1       |\n",
      "| policy_loss        | -4.98e+04 |\n",
      "| total_timesteps    | 34000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.01      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1800      |\n",
      "| policy_entropy     | 5.08      |\n",
      "| policy_loss        | -5.55e+04 |\n",
      "| total_timesteps    | 36000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | -3.1e-06  |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 1900      |\n",
      "| policy_entropy     | 5.09      |\n",
      "| policy_loss        | -5.18e+04 |\n",
      "| total_timesteps    | 38000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.04      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | 0         |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 2000      |\n",
      "| policy_entropy     | 3.97      |\n",
      "| policy_loss        | -3.99e+04 |\n",
      "| total_timesteps    | 40000     |\n",
      "| value_loss         | 9.89e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1         |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 2100      |\n",
      "| policy_entropy     | 4.01      |\n",
      "| policy_loss        | -4.28e+04 |\n",
      "| total_timesteps    | 42000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 2200      |\n",
      "| policy_entropy     | 4.03      |\n",
      "| policy_loss        | -2.96e+04 |\n",
      "| total_timesteps    | 44000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.07      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | 0         |\n",
      "| fps                | 324       |\n",
      "| nupdates           | 2300      |\n",
      "| policy_entropy     | 4.54      |\n",
      "| policy_loss        | -4.12e+04 |\n",
      "| total_timesteps    | 46000     |\n",
      "| value_loss         | 9.91e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.02      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 2400      |\n",
      "| policy_entropy     | 4.23      |\n",
      "| policy_loss        | -4.59e+04 |\n",
      "| total_timesteps    | 48000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 1.03      |\n",
      "| ep_reward_mean     | -1e+04    |\n",
      "| explained_variance | nan       |\n",
      "| fps                | 325       |\n",
      "| nupdates           | 2500      |\n",
      "| policy_entropy     | 4.54      |\n",
      "| policy_loss        | -4.68e+04 |\n",
      "| total_timesteps    | 50000     |\n",
      "| value_loss         | 9.92e+07  |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ACKTR('MlpPolicy', env, verbose=1).learn(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs= [[44 48]]\n",
      "==========\n",
      "Step 1\n",
      "Action:  [95]\n",
      "obs= [[246  48]] reward= [-10000.] done= [ True]\n",
      "Current taxi zone: 246, time: 48, reward: 0.00\n",
      "==========\n",
      "Goal reached! reward= [-10000.]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "print(\"obs=\", obs)\n",
    "print('==========')\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "    env.render(mode='console')\n",
    "    print('==========')\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
